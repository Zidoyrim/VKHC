AIM: Activation functions
# Sigmoid Function
import matplotlib.pyplot as plt
import numpy as np

def sigmoid(x):
    s = 1/(1+np.exp(-x))
    ds = s*(1-s)
    return s, ds

x = np.arange(-6,6,0.01)
sigmoid(x)
fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x,sigmoid(x)[0], color="#307EC7", linewidth=3, label="sigmoid")
ax.plot(x,sigmoid(x)[1], color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper right", frameon=False)
plt.show()

# relu activation and its derivative
import matplotlib.pyplot as plt
import numpy as np

def relu(x):
    r = np.maximum(0, x)
    dr = np.where(x <= 0, 0, 1)
    return r, dr

x = np.arange(-6, 6, 0.01)
relu_values, relu_derivatives = relu(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, relu_values, color="#307EC7", linewidth=3, label="ReLU")
ax.plot(x,relu_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper right", frameon=False)
plt.show()

# Leaky ReLU
import matplotlib.pyplot as plt
import numpy as np

def leaky_relu(x, alpha=0.01):
    r = np.maximum(alpha*x, x)
    dr = np.where(x < 0, alpha, 1)
    return r, dr

x = np.arange(-6, 6, 0.01)
leaky_relu_values, leaky_relu_derivatives = leaky_relu(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, leaky_relu_values, color="#307EC7", linewidth=1, label="Leaky ReLU")
ax.plot(x,leaky_relu_derivatives, color="#9621E2", linewidth=1, label="derivative")
ax.legend(loc="upper right", frameon=False)
plt.show()

# Parametric Rectified Linear Unit (PReLU)
import matplotlib.pyplot as plt
import numpy as np

def prelu(x, alpha=0.25):
    r = np.maximum(alpha*x, x)
    dr = np.where(x < 0, alpha, 1)
    return r, dr

x = np.arange(-6, 6, 0.01)
prelu_values, prelu_derivatives = prelu(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, prelu_values, color="#307EC7", linewidth=3, label="PReLU")
ax.plot(x, prelu_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper left", frameon=False)
plt.show()

# Exponential Linear Unit (ELU)
import matplotlib.pyplot as plt
import numpy as np

def elu(x, alpha=1.0):
    r = np.where(x >= 0, x, alpha*(np.exp(x)-1))
    dr = np.where(x >= 0, 1, alpha*np.exp(x))
    return r, dr

x = np.arange(-6, 6, 0.01)
elu_values, elu_derivatives = elu(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, elu_values, color="#307EC7", linewidth=3, label="ELU")
ax.plot(x, elu_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper left", frameon=False)
plt.show()

# SoftPlus
import matplotlib.pyplot as plt
import numpy as np

def softplus(x):
    r = np.log(1+np.exp(x))
    dr = 1/(1+np.exp(-x))
    return r, dr

x = np.arange(-6, 6, 0.01)
softplus_values, softplus_derivatives = softplus(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, softplus_values, color="#307EC7", linewidth=3, label="Softplus")
ax.plot(x, softplus_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper left", frameon=False)
plt.show()

# ArcTan
import matplotlib.pyplot as plt
import numpy as np

def arctan(x):
    r = np.arctan(x)
    dr = 1/(1+x**2)
    return r, dr

x = np.arange(-6, 6, 0.01)
arctan_values, arctan_derivatives = arctan(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, arctan_values, color="#307EC7", linewidth=3, label="Arctan")
ax.plot(x, arctan_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper left", frameon=False)
plt.show()

# TanH
import matplotlib.pyplot as plt
import numpy as np

def tanh(x):
    r = np.tanh(x)
    dr = 1 - r**2
    return r, dr

x = np.arange(-6, 6, 0.01)
tanh_values, tanh_derivatives = tanh(x)

fig, ax = plt.subplots(figsize=(9,5))
ax.spines['left'].set_position('center')
ax.spines['bottom'].set_position(('data', 0))
ax.spines['right'].set_color('none')
ax.spines['top'].set_color('none')
ax.xaxis.set_ticks_position('bottom')
ax.yaxis.set_ticks_position('left')

ax.plot(x, tanh_values, color="#307EC7", linewidth=3, label="Tanh")
ax.plot(x, tanh_derivatives, color="#9621E2", linewidth=3, label="derivative")
ax.legend(loc="upper left", frameon=False)
plt.show()

AIM:Autoendoders
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.layers import Input, Dense
from tensorflow.keras.models import Model
from tensorflow.keras.datasets import mnist

#load
(x_train, _), (x_test, _) = mnist.load_data()

#normalize
x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train),np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test),np.prod(x_test.shape[1:])))

#size
encoding_dim = 32
#input
input_img = Input(shape=(784,))
#encoded
encoded = Dense(encoding_dim, activation='relu')(input_img)
#decoded
decoded = Dense(784, activation='sigmoid')(encoded)
#this model maps its reconstruction
autoencoder = Model(input_img, decoded)
#this model maps its  encoded representation
encoder = Model(input_img, encoded)
#create
encoded_input = Input(shape=(encoding_dim,))
#retrive
decoder_layer =autoencoder.layers[-1]
#create
decoder = Model(encoded_input, decoder_layer(encoded_input))

autoencoder.compile(optimizer='adam', loss='binary_crossentropy')

autoencoder.fit(x_train, x_train,
               epochs=50,
               batch_size=256,
               shuffle=True,
               validation_data=(x_test, x_test))
#encode
encoded_imgs = encoder.predict(x_test)
decoded_imgs = decoder.predict(encoded_imgs)
#use
n=10
plt.figure(figsize=(20,4))
for i in range(n):
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(28, 28))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
plt.show()

AIM: CNN 
# Implement CNN for Digit Recognition on the MNIST Dataset

import tensorflow as tf
from tensorflow import keras
import numpy as np
import matplotlib.pyplot as plt

(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()

x_train = x_train.astype("float32")/255.0
x_test = x_test.astype("float32")/255.0
x_train = np.expand_dims(x_train, -1)
x_test = np.expand_dims(x_test, -1)

model = keras.models.Sequential([
    keras.layers.Conv2D(32, (3,3), activation="relu", input_shape=(28,28,1)),
    keras.layers.MaxPooling2D((2,2)),
    keras.layers.Conv2D(64, (3,3), activation="relu"),
    keras.layers.MaxPooling2D((2,2)),
    keras.layers.Flatten(),
    keras.layers.Dense(64, activation="relu"),
    keras.layers.Dense(10, activation="softmax")
])

model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=['accuracy'])

history = model.fit(x_train, y_train, epochs=15, batch_size=128, validation_data=(x_test, y_test))

test_loss, test_acc = model.evaluate(x_test, y_test)
print("Test accuracy:", test_acc)

sample_img = x_test[0]
sample_label = y_test[0]
sample_img = np.expand_dims(sample_img, 0)
pred = model.predict(sample_img)
pred_label = np.argmax(pred)
print("Sample image true label:", sample_label)
print("Sample image predicted label:", pred_label)

plt.imshow(sample_img.squeeze(), cmap='gray')
plt.show()

AIM: LSTM_GRU
#LSTM
import yfinance as yf
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, GRU, Dense
stock_symbol = 'AAPL'
df = yf.download(stock_symbol, start='2010-01-01', end='2023-01-01')

data = df[['Close']].values
scaler = MinMaxScaler(feature_range=(0,1))
data_scaled = scaler.fit_transform(data)

train_size = int(len(data_scaled)*0.8)
train, test = data_scaled[0:train_size], data_scaled[train_size:]

def create_dataset(dataset, look_back=60):
    X, Y = [], []
    for i in range(len(dataset)-look_back):
        X.append(dataset[i:i+look_back, 0])
        Y.append(dataset[i + look_back, 0])
    return np.array(X), np.array(Y)

look_back = 60
X_train, Y_train = create_dataset(train, look_back)
X_test, Y_test = create_dataset(test, look_back)

X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))
X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], 1))

model_lstm = Sequential()
model_lstm.add(LSTM(50, input_shape=(look_back, 1)))
model_lstm.add(Dense(1))
model_lstm.compile(loss='mean_squared_error', optimizer='adam')

model_gru = Sequential()
model_gru.add(GRU(50, input_shape=(look_back, 1)))
model_gru.add(Dense(1))
model_gru.compile(loss='mean_squared_error', optimizer='adam')

model_lstm.fit(X_train, Y_train, epochs=20, batch_size=32, verbose=1)
model_gru.fit(X_train, Y_train, epochs=20, batch_size=32, verbose=1)

lstm_predictions = model_lstm.predict(X_test)
gru_predictions = model_gru.predict(X_test)

lstm_predictions = scaler.inverse_transform(lstm_predictions)
gru_predictions = scaler.inverse_transform(gru_predictions)
Y_test_actual = scaler.inverse_transform([Y_test])

future_steps = 30
last_sequence = X_test[-1]

lstm_future_predictions = []
for _ in range(future_steps):
    prediction = model_lstm.predict(np.reshape(last_sequence, (1, look_back, 1)))
    lstm_future_predictions.append(prediction[0][0])
    last_sequence = np.append(last_sequence[1:], prediction[0])

last_sequence = X_test[-1]
gru_future_predictions = []
for _ in range(future_steps):
    prediction = model_gru.predict(np.reshape(last_sequence, (1, look_back, 1)))
    gru_future_predictions.append(prediction[0][0])
    last_sequence = np.append(last_sequence[1:], prediction[0])

lstm_future_predictions = scaler.inverse_transform(np.array(lstm_future_predictions).reshape(-1, 1))
gru_future_predictions = scaler.inverse_transform(np.array(gru_future_predictions).reshape(-1, 1))

future_range = np.arange(len(Y_test_actual[0]), len(Y_test_actual[0])+ future_steps)

plt.figure(figsize=(14,7))
plt.plot(Y_test_actual[0], label='Actual Stock Price', color='blue')
plt.plot(lstm_predictions, label='LSTM Predictions', color='orange')
plt.plot(gru_predictions, label='GRU Predictions', color='green')

plt.plot(future_range, lstm_future_predictions, label='LSTM Future Predictions', color='red')
plt.plot(future_range, gru_future_predictions, label='GRU Future Predictions', color='purple')

plt.title(f'{stock_symbol} Stock Price Prediction and Future Forecasting - LSTM vs GRU')
plt.xlabel('Time Steps')
plt.ylabel('Stock Price')
plt.legend()
plt.show()

AIM: Optimizers
# Gradient Based Optimization
import tensorflow as tf
import matplotlib.pyplot as plt

def loss_function(x):
    return (x-3)**2

x = tf.Variable(initial_value = 5.0, trainable = True, dtype = tf.float32)
learning_rate = 0.1
steps = []
loss_values = []

for step in range(20):
    with tf.GradientTape() as tape:
        loss = loss_function(x)

    gradients = tape.gradient(loss, [x])

    x.assign_sub(learning_rate*gradients[0])

    steps.append(step)
    loss_values.append(loss.numpy())

    print(f"Step {step+1}: x = {x.numpy():.4f}, Loss = {loss.numpy():.4f}")

plt.plot(steps, loss_values, marker='o')
plt.title('Gradient-Based Optimization: :Loss Reduction')
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

# Using Tensorflow Optimizers
x = tf.Variable(initial_value = 5.0, trainable = True, dtype = tf.float32)

optimizer = tf.keras.optimizers.Adam(learning_rate=0.1)
adam_steps = []
adam_loss_values = []

for step in range(20):
    with tf.GradientTape() as tape:
        loss = loss_function(x)

    gradients = tape.gradient(loss, [x])

    optimizer.apply_gradients(zip(gradients, [x]))

    adam_steps.append(step)
    adam_loss_values.append(loss.numpy())

    print(f"Step {step+1} (Adam): x = {x.numpy():.4f}, Loss = {loss.numpy():.4f}")

plt.plot(adam_steps, adam_loss_values, marker='o', color='red')
plt.title('Adam Optimization: Loss Reduction')
plt.xlabel('Steps')
plt.ylabel('Loss')
plt.grid('True')
plt.show()

import tensorflow as tf
from tensorflow.keras import layers, models
import matplotlib.pyplot as plt

(mnist_train, mnist_train_labels), (mnist_test, mnist_test_labels) = tf.keras.datasets.mnist.load_data()
mnist_train = mnist_train/255.0
mnist_test = mnist_test/255.0

mnist_train = mnist_train[..., tf.newaxis]
mnist_test = mnist_test[..., tf.newaxis]
print(f"MNIST Train Shape: {mnist_train.shape}, MNIST Test Shape: {mnist_test.shape}")

batch_size = 32
train_dataset = tf.data.Dataset.from_tensor_slices((mnist_train, mnist_train_labels))
train_dataset = train_dataset.shuffle(buffer_size=10000).batch(batch_size)

test_dataset = tf.data.Dataset.from_tensor_slices((mnist_test, mnist_test_labels))
test_dataset = test_dataset.batch(batch_size)

def augment(image, label):
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_brightness(image, max_delta=0.1)
    return image, label

augmented_train_dataset = train_dataset.map(augment)
model = models.Sequential([
    layers.Input(shape=(28,28,1)),
    layers.Conv2D(32, kernel_size=(3,3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2,2)),
    layers.Conv2D(64, kernel_size=(3,3), activation='relu'),
    layers.MaxPooling2D(pool_size=(2,2)),
    layers.Flatten(),
    layers.Dense(128, activation='relu'),
    layers.Dense(10, activation='softmax')
])

model.compile(optimizer='adam',
              loss='sparse_categorical_crossentropy',
              metrics=['accuracy'])

history = model.fit(train_dataset, epochs=10, validation_data=test_dataset)

plt.figure(figsize=(12,4))
plt.subplot(1,2,1)
plt.plot(history.history['accuracy'], label=['Train Accuracy'])
plt.plot(history.history['val_accuracy'], label=['Validation Accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.subplot(1,2,2)
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

AIM: Regularizations to prevent overfitting
#Regularization to prevent the model from Overfitting

import tensorflow as tf

# Load MNIST data
(train_data, train_labels), (test_data, test_labels) = tf.keras.datasets.mnist.load_data()

# Normalize and flatten the images
train_data = train_data.reshape((60000, 784)) / 255.0
test_data = test_data.reshape((10000, 784)) / 255.0

# One-hot encode labels
train_labels = tf.keras.utils.to_categorical(train_labels, 10)
test_labels = tf.keras.utils.to_categorical(test_labels, 10)

# Build the model
model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(128, activation='relu', input_shape=(784,), kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.01)),
    tf.keras.layers.Dense(10, activation='softmax')
])

# Compile the model
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# Train the model
history = model.fit(train_data, train_labels, epochs=10, batch_size=128, validation_data=(test_data, test_labels))

import matplotlib.pyplot as plt
plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()

plt.subplot(1,2,2)
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Training and Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()

plt.tight_layout()
plt.show()

AIM: RNN
#RNN
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, SimpleRNN, LSTM
from sklearn.model_selection import train_test_split
import seaborn as sns
url= 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/airline-passengers.csv'
df = pd.read_csv(url,usecols=['Passengers'])
df.head()
scaler = MinMaxScaler(feature_range=(0,1))
scaled_data = scaler.fit_transform(df)

def create_dataset(data, time_step=1):
    X, Y = [], []
    for i in range(len(data)-time_step-1):
        a = data[i:(i+time_step),0]
        X.append(a)
        Y.append(data[i+time_step,0])
    return np.array(X), np.array(Y)

time_step=10
X, Y = create_dataset(scaled_data, time_step)
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)

X_train = X_train.reshape(X_train.shape[0], X_train.shape[1],1)
X_test = X_test.reshape(X_test.shape[0], X_test.shape[1],1)

model = Sequential()
model.add(SimpleRNN(50, return_sequences=True, input_shape=(time_step,1)))
model.add(SimpleRNN(50, return_sequences=False))
model.add(Dense(25))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mean_squared_error')

model.fit(X_train, Y_train, epochs=100, batch_size=32, validation_data=(X_test, Y_test), verbose=1)
train_predict = model.predict(X_train)
test_predict = model.predict(X_test)

train_predict = scaler.inverse_transform(train_predict)
test_predict = scaler.inverse_transform(test_predict)
Y_train = scaler.inverse_transform([Y_train])
Y_test = scaler.inverse_transform([Y_test])

train_rmse = np.sqrt(mean_squared_error(Y_train[0], train_predict[:,0]))
test_rmse = np.sqrt(mean_squared_error(Y_test[0], test_predict[:,0]))

print(f'Train RMSE: {train_rmse}')
print(f'Test RMSE: {test_rmse}')
from statsmodels.tsa.arima.model import ARIMA

model_arima = ARIMA(df, order=(5,1,0))
model_arima_fit = model_arima.fit()

arima_pred = model_arima_fit.forecast(steps=len(X_test))

arima_rmse = np.sqrt(mean_squared_error(df[-len(arima_pred):], arima_pred))
print(f' ARIMA RMSE: {arima_rmse}')

plt.figure(figsize=(12,6))
plt.plot(df, label='Actual', color='blue')
plt.plot(range(time_step, time_step+len(train_predict)), train_predict, color='green', label='Train Predictions')
plt.plot(range(len(df)-len(test_predict), len(df)), test_predict, color='red', label='Test Predictions')

plt.legend()
plt.title('RNN Predictions vs. Actual')
plt.xlabel('Time')
plt.ylabel('Passengers')
plt.show()
plt.figure(figsize=(12,6))
plt.plot(df, label='Actual')
plt.plot(range(len(df)-len(arima_pred), len(df)), arima_pred, color='green', label='ARIMA Prediction')
plt.legend()
plt.title('ARIMA Predictions vs. Actual')
plt.xlabel('Time')
plt.ylabel('Passengers')
plt.show()

AIM: Transfer Learing
pip install tensorflow
import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
import os
import zipfile
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import VGG16

# Define cache directory and filename
cache_dir = os.getcwd()  # Set the current working directory as the cache directory
filename = "cats_and_dogs_filtered.zip"  # Name of the file to be downloaded
url = "https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip"

# Download the file
file_path = tf.keras.utils.get_file(filename, url, cache_dir=cache_dir)

# Extract the file
with zipfile.ZipFile(file_path, "r") as zip_ref:
    zip_ref.extractall()

# Define directories
train_dir = os.path.join(os.getcwd(), "cats_and_dogs_filtered", "train")
validation_dir = os.path.join(os.getcwd(), "cats_and_dogs_filtered", "validation")

# Data augmentation and rescaling for training and validation data
train_datagen = ImageDataGenerator(rescale=1./255,
                                  rotation_range=20,
                                  width_shift_range=0.2,
                                  height_shift_range=0.2,
                                  shear_range=0.2,
                                  zoom_range=0.2,
                                  horizontal_flip=True)
validation_datagen = ImageDataGenerator(rescale=1./255)

# Image generators
train_generator = train_datagen.flow_from_directory(train_dir,
                                                   target_size=(150,150),
                                                   batch_size=20,
                                                   class_mode="binary")
validation_generator = validation_datagen.flow_from_directory(validation_dir,
                                                   target_size=(150,150),
                                                   batch_size=20,
                                                   class_mode="binary")

# Load pre-trained VGG16 model
conv_base = VGG16(weights="imagenet",
                 include_top=False,
                  input_shape=(150, 150, 3))

# Freeze the convolutional base
conv_base.trainable = False

# Build the model
model = tf.keras.models.Sequential()
model.add(conv_base)
model.add(tf.keras.layers.Flatten())
model.add(tf.keras.layers.Dense(256, activation="relu"))
model.add(tf.keras.layers.Dropout(0.5))
model.add(tf.keras.layers.Dense(1, activation="sigmoid"))

# Compile the model
model.compile(loss="binary_crossentropy",
             optimizer=tf.keras.optimizers.RMSprop(learning_rate=2e-5),
             metrics=["accuracy"])

# Train the model
history = model.fit(train_generator,
                   steps_per_epoch=100,
                   epochs=30,
                   validation_data=validation_generator,
                   validation_steps=50)

# Display predictions for some validation images
x, y_true = next(validation_generator)
y_pred = model.predict(x)
class_names = ['cat', 'dog']
for i in range(len(x)):
    plt.imshow(x[i])
    plt.title(f'Predicted class: {class_names[int(round(y_pred[i][0]))]}, True class: {class_names[int(y_true[i])]}')
    plt.show()

# Plot training and validation accuracy
acc = history.history["accuracy"]
val_acc = history.history["val_accuracy"]
loss = history.history["loss"]
val_loss = history.history["val_loss"]

epochs = range(1, len(acc) + 1)

# Accuracy plot
plt.plot(epochs, acc, "bo", label="Training acc")
plt.plot(epochs, val_acc, "b", label="Validation acc")
plt.title("Training and validation accuracy")
plt.legend()

# Loss plot
plt.figure()
plt.plot(epochs, loss, "bo", label="Training loss")
plt.plot(epochs, val_loss, "b", label="Validation loss")
plt.title("Training and validation loss")
plt.legend()

plt.show()

