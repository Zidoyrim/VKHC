Practical 1
Aim:- Face Detection
A) Code:-
# detect face from input image and save it on the disk.
import cv2
# Load the pre-trained Haar Cascade model for face detection 
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
# load the image where you want to detect face
image_path = 'image.jpg'  # path to your image
image = cv2.imread(image_path)
# convert the image to grayscale
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
# detect faces in the image
faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30,30))
# draw rectangles around each face
for (x,y,w,h) in faces:
    cv2.rectangle(image,(x,y),(x+w,y+h),(255,0,0),2)
# save the image with faces highlighted 
output_path = 'faces_detected.jpg'   # corrected file extension
cv2.imwrite(output_path, image)
print(f"Faces Detected: {len(faces)}. Output saved to {output_path}")

B) Code:-
# detect faces and show it on screen
import cv2
import matplotlib.pyplot as plt
# initialize the Haar Cascade face detection model
face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')
# start capturing video from the camera (default camera is usually at index 0)
cap = cv2.VideoCapture(0)
# Capture a single frame
ret, frame = cap.read()
if ret:  # check if the frame was successfully captured
    # convert the captured frame to grayscale
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    # detect faces in the grayscale frame
    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30,30))
    # draw rectangles around the detected faces
    for (x,y,w,h) in faces:
        cv2.rectangle(frame, (x,y), (x+w, y+h), (0,255,0), 2)
    # Display the resulting frame with faces highlighted using matplotlib
    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    plt.axis('off')  # turn off axis labels
    plt.show()
# Release the capture
cap.release()
print('Number of faces detected: ',len(faces))











Practical 2
Aim:- Pedestrians Detection
A) Code for Ideal Script:-
import cv2
import matplotlib.pyplot as plt
import matplotlib.animation as animation
# Initialize the Haar Cascade pedestrian detection model
pedestrians_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_fullbody.xml')
# Load the video
video_path = 'C:/Users/DELL/OneDrive/Desktop/MSc IT-1/video.mp4'
cap = cv2.VideoCapture(video_path)
# Check if the video is opened successfully
if not cap.isOpened():
    print("Error: Unable to open the video")
    exit()
# Function to detect pedestrians and draw bounding boxes
def detect_pedestrians(frame):
    # Convert the frame to grayscale
    gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    # Detect pedestrians in the grayscale frame
    pedestrians = pedestrians_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30,30))
    # Draw rectangle around the detected pedestrians
    for (x,y,w,h) in pedestrians:
        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)
    return frame
# Create a subplot for displaying the video frame
fig, ax = plt.subplots()
# Function to update the animation
def update(frame):
    # Read a frame from the video
    ret, frame = cap.read()
    if not ret:
        ani.event_source.stop()
        return
    # Detect pedestrians and draw bounding boxes
    frame_with_pedestrians = detect_pedestrians(frame)
    # Display the frame
    ax.clear()
    ax.imshow(cv2.cvtColor(frame_with_pedestrians, cv2.COLOR_BGR2RGB))
    ax.axis('off')   # Turn off axis labels
# Create the animation
ani = animation.FuncAnimation(fig, update, interval=50)
plt.show()
# Release the video capture object
cap.release()

B) Code for Jupyter:-
import cv2
import matplotlib.pyplot as plt
# Initialize the Haar Cascade pedestrian detection model
pedestrian_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_fullbody.xml')
# Load the video
video_path = 'C:/Users/DELL/OneDrive/Desktop/MSc IT-1/video.mp4'
cap = cv2.VideoCapture(video_path)
# Check if the video is opened successfully
if not cap.isOpened():
    print("Error: Unable to open the video")
    exit()
# Loop through the video frames
while True:
    # Read a frame from the video
    ret, frame = cap.read()
    if not ret:
        break   # Break the loop if no frame is captured
    # Convert the frame to grayScale
    gray_frame = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY)
    # Detect pedestrians in the grayscale frame
    pedestrians = pedestrian_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5, minSize=(30,30))
    # Draw rectangle around the detected pedestrians
    for (x,y,w,h) in pedestrians:
        cv2.rectangle(frame, (x,y), (x+w,y+h), (0,255,0), 2)
    
    # Display the resulting frame with pedestrians highlighted using Matplotlib
    plt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
    plt.axis('off')   # Turn off the axis
    plt.show()
    # Check if the loop should be exited
    # In Jupyter Notebook you might need to manually interrupt the kernel to stop the loop
# Release the video capture object
cap.release()


Prac 3 
#Aim: Perform Camera Calibration 
#code:

import cv2
import numpy as np
import matplotlib.pyplot as plt

# Termination criteria for cornerSubPix
criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 30, 0.001)

# Prepare object points (0,0,0), (1,0,0), (2,0,0),..., (6,5,0)
objp = np.zeros((5*7, 3), np.float32)
objp[:, :2] = np.mgrid[0:7, 0:5].T.reshape(-1,2)

# Arrays to store object points and image points from all images
objpoints = []
imgpoints = []

# List of calibration images
calibration_images = [
    'calibration_images/camera_calib1.jpg',
    'calibration_images/camera_calib2.jpg',
    'calibration_images/camera_calib3.jpg',
    'calibration_images/camera_calib4.jpg',
    'calibration_images/camera_calib5.jpg',
    'calibration_images/camera_calib6.jpg',
    'calibration_images/camera_calib7.jpg',
    'calibration_images/camera_calib8.jpg',
    'calibration_images/camera_calib9.jpg',
    'calibration_images/camera_calib10.jpg',
]

# Step through the list and search for chessboard corners
for fname in calibration_images:
    img = cv2.imread(fname)
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    # Find the chessboard corners
    ret, corners = cv2.findChessboardCorners(gray, (5,7), None)

    # If found, add object points, image points
    if ret == True:
        objpoints.append(objp)
        corners2 = cv2.cornerSubPix(gray, corners, (11,11), (-1,-1), criteria)
        imgpoints.append(corners2)

        # Draw and display the corners
        img = cv2.drawChessboardCorners(img, (5,7), corners2, ret)

# Calibrate camera
ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)

# Undistort an image
img = cv2.imread(calibration_images[0])
h, w = img.shape[:2]
newcamerantx, roi = cv2.getOptimalNewCameraMatrix(mtx, dist, (w,h), 1, (w,h))
dst = cv2.undistort(img, mtx, dist, None, newcamerantx)

# Crop the image
x, y, w, h = roi
dst = dst[y:y+h, x:x+w]

# Plot original and undistorted images
fig, ax = plt.subplots(1, 2, figsize=(10,5))
ax[0].imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))
ax[0].set_title('Original Image')
ax[0].axis('off')
ax[1].imshow(cv2.cvtColor(dst, cv2.COLOR_BGR2RGB))
ax[1].set_title('Undistorted Image')
ax[1].axis('off')
plt.show()







Practical 3
Aim:- Object Detection
Code:-
import argparse
import numpy as np
import cv2
import matplotlib.pyplot as plt

args = argparse.Namespace(
    image="dog.jpg",
    weights="yolov3.weights",
    config="yolov3.cfg",
    classes="yolov3.txt"
)
def get_output_layers(net):
    layer_names = net.getLayerNames()
    try:
        output_layers = [layer_names[i-1] for i in net.getUnconnectedOutLayers()]
    except:
        output_layers = [layer_names[i-1] for i in net.getUnconnectedOutLayers()]
    return output_layers

def draw_prediction(img, class_id, confidence, x, y, x_plus_w, y_plus_h):
    label = str(classes[class_id])
    color = COLORS(class_id)
    cv2.rectangle(img, (x,y),(x_plus_w, y_plus_h), label, color,2)
    cv2.putText(img, label,  (x-10,y-10),cv2.FONT_HERSHEY_COMPLEX, color, 2)
# read input image
image = cv2.imread(args.image)

Width = image.shape[1]
Height = image.shape[0]
scale = 0.00392
classes = None

# read class names from text file
classes = None
with open(args.classes, 'r') as f:
    classes = [line.strip() for line in f.readlines()]

# generate different colors for different classes 
COLORS = np.random.uniform(0, 255, size=(len(classes), 3))

# read pre-trained model and config file
net = cv2.dnn.readNet(args.weights, args.config)

# create input blob 
blob = cv2.dnn.blobFromImage(image, scale, (416,416), (0,0,0), True, crop=False)

# set input blob for the network
net.setInput(blob)

# function to get the output layer names 
# in the architecture

# function to draw bounding box on the detected object with class name
def draw_bounding_box(img, class_id, confidence, x, y, x_plus_w, y_plus_h):

    label = str(classes[class_id])

    color = COLORS[class_id]

    cv2.rectangle(img, (x,y), (x_plus_w,y_plus_h), color, 2)

    cv2.putText(img, label, (x-10,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)

# run inference through the network
# and gather predictions from output layers
outs = net.forward(get_output_layers(net))

# initialization
class_ids = []
confidences = []
boxes = []
conf_threshold = 0.5
nms_threshold = 0.4

# for each detetion from each output layer 
# get the confidence, class id, bounding box params
# and ignore weak detections (confidence < 0.5)
for out in outs:
    for detection in out:
        scores = detection[5:]
        class_id = np.argmax(scores)
        confidence = scores[class_id]
        if confidence > 0.5:
            center_x = int(detection[0] * Width)
            center_y = int(detection[1] * Height)
            w = int(detection[2] * Width)
            h = int(detection[3] * Height)
            x = center_x - w / 2
            y = center_y - h / 2
            class_ids.append(class_id)
            confidences.append(float(confidence))
            boxes.append([x, y, w, h])


# apply non-max suppression
indices = cv2.dnn.NMSBoxes(boxes, confidences, conf_threshold, nms_threshold)

# go through the detections remaining
# after nms and draw bounding box
for i in indices:
    try: 
        box=boxes[i]
    except:
        i=i[0]
        box=boxes[i]
 
    x = box[0]
    y = box[1]
    w = box[2]
    h = box[3]
    draw_bounding_box(image, class_ids[i], confidences[i], round(x), round(y), round(x+w), round(y+h))

# display output image    
#cv2.imshow("object detection", image)

plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))
plt.axis('off')  # turn off axis labels
plt.show()
# wait until any key is pressed
#cv2.waitKey()
    
 # save output image to disk
#cv2.imwrite("object-detection.jpg", image)

# release resources
#cv2.destroyAllWindows()





















Practical 4
Aim:- Image Stitching
Code:-
import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load images
img_ = cv2.imread('right.jpg')
img1 = cv2.cvtColor(img_, cv2.COLOR_BGR2GRAY)
img = cv2.imread('left.jpg')
img2 = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

#Initialize SIFT detector
sift = cv2.SIFT_create()
kp1, des1 = sift.detectAndCompute(img1,None)
kp2, des2 = sift.detectAndCompute(img2,None)

# Create a BFMatcher object with distance measurement cv2.NORM_L2
bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck = False)

# Perfomr the matching between the SIFT descriptors of the images
matches = bf.knnMatch(des1,des2,k=2)

# Apply the ratio test to find good matches
good = []
for m,n in matches:
    if m.distance < 0.75*n.distance:
        good.append(m)

# Atleast 4 matches are to be there to find the homography
if len(good)>4:
    # prepare source and destination points
    src_pts = np.float32([kp1[m.queryIdx].pt for m in good]).reshape(-1,1,2)
    dst_pts = np.float32([kp2[m.trainIdx].pt for m in good]).reshape(-1,1,2)
    
    # Compute Homography
    H, status = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
    
    # Use homography to warp image
    dst = cv2.warpPerspective(img_,H, (img.shape[1]+img_.shape[1],img.shape[0]))
    
    # Convert warped image from BGR to RGB for matplotlib
    dst_rgb = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)
    
    # Display the warped image
    plt.subplot(122), plt.imshow(dst_rgb), plt.title('Warped Image')
    plt.show()
    
    # Place the left image on the appropriate position 
    dst[0:img.shape[0], 0:img.shape[1]] = img
    
    # Convert the combined image from BGR to RGB for matplotlib
    combined_rgb = cv2.cvtColor(dst, cv2.COLOR_BGR2RGB)
    
    # Save the stitched image as output.jpg in the BGR format
    cv2.imwrite('output.jpg',dst)
    
    # Display the stitched image
    plt.imshow(combined_rgb)
    plt.title('Stitched Image')
    plt.show()
    
else:
    raise AssertionError('Not enough matches are found - {}/{}'.format(len(good),4))


prac 5
prac5 Aim: Construct 3D model from Images  
import cv2 
import numpy as np 
import matplotlib.pyplot as plt 
from mpl_toolkits.mplot3d import Axes3D 
def extract_keypoints_and_descriptors(image): 
    sift = cv2.SIFT_create() 
    keypoints, descriptors = sift.detectAndCompute(image, None) 
    return keypoints, descriptors 
def match_keypoints(desc1, desc2): 
    bf = cv2.BFMatcher(cv2.NORM_L2, crossCheck = True) 
    matches = bf.match(desc1, desc2) 
    matches = sorted(matches, key = lambda x : x.distance) 
    return matches
def draw_matches(img1, kp1, img2, kp2, matches): 
    return cv2.drawMatches(img1, kp1, img2, kp2, matches[:50], None, flags = 
cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS) 
def reconstruct_3d_points(K, kp1, kp2, matches, E): 
    points1 = np.float32([kp1[m.queryIdx].pt for m in matches]) 
    points2 = np.float32([kp2[m.trainIdx].pt for m in matches]) 
    _, R, t, mask = cv2.recoverPose(E, points1, points2, K) 
    # Triangulate points 
    P1 = np.hstack((np.eye(3,3), np.zeros((3,1)))) 
    P2 = np.hstack((R,t)) 
    points_4d_hom = cv2.triangulatePoints(P1, P2, points1.T, points2.T) 
    points_4d = points_4d_hom / np.tile(points_4d_hom[-1,:], (4,1)) 
    points_3d = points_4d[:3, :].T 
    return points_3d 
# Load two images of a scene 
img1 = cv2.imread('3D1.jpg', cv2.IMREAD_GRAYSCALE) 
img2 = cv2.imread('3D2.jpg', cv2.IMREAD_GRAYSCALE) 
# Placeholder values for the camera matrix K  
fx = 1000  # focal length in pixels 
fy = 1000  # focal length in pixels 
cx = img1.shape[1] / 2  # Optical center X coordinate, assuming center of the image 
cy = img1.shape[0] / 2  # Optical center Y coordinate, assuming center of the image 
K = np.array([[fx, 0, cx], 
             [0, fy, cy], 
             [0, 0, 1]], dtype=float) 
# Extract keypoints and descriptors 
kp1, desc1 = extract_keypoints_and_descriptors(img1) 
kp2, desc2 = extract_keypoints_and_descriptors(img2) 
# Match Keypoints 
matches = match_keypoints(desc1, desc2)
# Estimate the essential matrix 
points1 = np.float32([kp1[m.queryIdx].pt for m in matches]) 
points2 = np.float32([kp2[m.trainIdx].pt for m in matches]) 
F, mask = cv2.findFundamentalMat(points1, points2, cv2.FM_RANSAC) 
E = K.T @ F @ K 
# Reconstruct 3D points 
points_3d = reconstruct_3d_points(K, kp1, kp2, matches, E) 
# Visualize 3D points 
fig = plt.figure() 
ax = fig.add_subplot(111, projection = '3d') 
ax.scatter(points_3d[:, 0], points_3d[:, 1], points_3d[:, 2]) 
plt.show() 
# Display matched keypoints 
matched_img = draw_matches(img1, kp1, img2, kp2, matches) 
plt.imshow(matched_img) 
plt.show()


prac 6 
#Aim: Implement object detection and tracking from video 
#Code: 
import cv2 
import numpy as np 
from IPython.display import display, clear_output 
from matplotlib import pyplot as plt 
# Load YOLO 
net = cv2.dnn.readNet("yolov3.weights", "yolov3.cfg") 
classes = [] 
with open("coco.names", "r") as f: 
    classes = [line.strip() for line in f.readlines()] 
layer_names = net.getLayerNames() 
output_layers = [layer_names[i-1] for i in net.getUnconnectedOutLayers()] 
# loading the video 
cap = cv2.VideoCapture('video2.mp4')
try: 
    while cap.isOpened(): 
        ret, frame = cap.read() 
        if not ret: 
            break 
        height, width, channels = frame.shape 
        # Detecting objects 
        blob = cv2.dnn.blobFromImage(frame, 0.00392, (416,416), (0,0,0), True, crop=False) 
        net.setInput(blob) 
        outs = net.forward(output_layers) 
        # Information to show on the screen (class_id, confidence, bounding boxes) 
        class_ids = [] 
        confidences = [] 
        boxes = [] 
        for out in outs: 
            for detection in out: 
                scores = detection[5:] 
                class_id = np.argmax(scores) 
                confidence = scores[class_id] 
                if confidence > 0.5: 
                    # Object Detected 
                    center_x = int(detection[0]*width) 
                    center_y = int(detection[1]*height) 
                    w = int(detection[2]*width) 
                    h = int(detection[3]*height) 
                    # Rectangle coordinates 
                    x = int(center_x - w/2) 
                    y = int(center_y - h/2) 
                    boxes.append([x, y, w, h]) 
                    confidences.append(float(confidence))
                    class_ids.append(class_id) 
        indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4) 
        for i in range(len(boxes)): 
            if i in indexes: 
                x, y, w, h = boxes[i] 
                label = str(classes[class_ids[i]]) 
                color = (0,255,0) 
                cv2.rectangle(frame, (x,y), (x+w,y+h), color, 2) 
                cv2.putText(frame, label, (x,y+30), cv2.FONT_HERSHEY_PLAIN, 1, color, 2) 
        # Convert the frame to RGB and display it 
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB) 
        plt.figure(figsize=(10,10)) 
        plt.imshow(frame_rgb) 
        plt.axis('off') 
        display(plt.gcf()) 
        clear_output(wait=True) 
        plt.close()       
finally: 
    cap.release() 
    print('Stream ended.')

Other code:
from ultralytics import YOLO
import cv2
import cvzone
import math

cap = cv2.VideoCapture(0)
cap.set(3, 1280)
cap.set(4, 720)
#cap = cv2.VideoCapture("Downloads/EP.1082.v0.1699152007.1080p.mp4")
model = YOLO("../Weights/yolov8n.pt")

classNames = ['person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',  'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush']

while True:
    success, img = cap.read()
    results = model.track(img, device="mps", stream=True) 
    for r in results:
        boxes = r. boxes
    for box in boxes:
        # Bounding Box
        x1, y1, x2, y2 = box. xyxy[0]
        x1, y1, x2, y2 = int(x1), int(y1), int(x2), int(y2)
        # cv2. rectangle(img, (x1, y1), (x2, y2), (255,0,255) , 3)
        w, h = x2 - x1, y2 - y1
        cvzone.cornerRect(img, (x1, y1, w, h))
        
        # Confidence
        conf = math.ceil((box.conf[0] * 100)) / 100
        cls = int(box.cls[0])
        cvzone.putTextRect(img, f'{classNames[cls]} {conf}', (max(0, x1), max(35, y1)))
        
    cv2.imshow("Image", img)
    cv2.waitKey(1)

#prac7
#Aim: A) Perform Feature extraction using RANSAC 
#code:
import cv2 
import numpy as np 
import matplotlib.pyplot as plt 
# Load the color images 
img1_color = cv2.imread('3D1.jpg')  # Query image (color) 
img2_color = cv2.imread('3D2.jpg')  # Train image (color) 
# Convert to gray scale for feature detection 
img1 = cv2.cvtColor(img1_color, cv2.COLOR_BGR2RGB) 
img2 = cv2.cvtColor(img2_color, cv2.COLOR_BGR2RGB) 
# Initialize SIFT detector 
sift = cv2.SIFT_create() 
# Detect keypoints and compute descriptors 
kp1, des1 = sift.detectAndCompute(img1, None) 
kp2, des2 = sift.detectAndCompute(img2, None) 
# FLANN paramters and matcher setup 
FLANN_INDEX_KDTREE = 1 
index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5) 
search_params = dict(checks = 50)
flann = cv2.FlannBasedMatcher(index_params, search_params) 
# Match descriptors using KNN 
matches = flann.knnMatch(des1, des2, k = 2) 
# Ratio test to keep good matches 
good = [] 
for m,n in matches: 
    if m.distance < 0.7*n.distance: 
        good.append(m) 
# Extract location of good matches 
points1 = np.zeros((len(good), 2), dtype=np.float32) 
points2 = np.zeros((len(good), 2), dtype=np.float32) 
for i, match in enumerate(good): 
    points1[i, :] = kp1[match.queryIdx].pt 
    points2[i, :] = kp2[match.trainIdx].pt 
# Find Homography using RANSAC 
H, status = cv2.findHomography(points1, points2, cv2.RANSAC) 
# Create a new image that puts the two images side by side 
height = max(img1_color.shape[0], img2_color.shape[0]) 
width = img1_color.shape[1] + img2_color.shape[1] 
output = np.zeros((height, width, 3), dtype = np.uint8) 
# Place both images within this new image 
output[0:img1_color.shape[0], 0:img1_color.shape[1]] = img1_color 
output[0:img2_color.shape[0], 
img1_color.shape[1]:img1_color.shape[1]+img2_color.shape[1]] = img2_color 
# Draw lines between the matching points 
for i, (m, color) in enumerate(zip(good, np.random.randint(0, 255, (len(good), 3)))): 
    if status[i]: 
        pt1 = tuple(map(int, kp1[m.queryIdx].pt)) 
        pt2 = tuple(map(int, kp2[m.trainIdx].pt)) 
        pt2 = (pt2[0] + img1_color.shape[1], pt2[1])  # Shift the point for img2
        cv2.line(output, pt1, pt2, color.tolist(), 2) 
# Convert the result to RGB for matplotlib display and show the final image 
output_rgb = cv2.cvtColor(output, cv2.COLOR_BGR2RGB) 
# Use matplotlib to display the image 
plt.figure(figsize=(15,5)) 
plt.imshow(output_rgb) 
plt.axis('off') 
plt.title('Feature Matching with RANSAC') 
plt.show()

prac 8
#Aim: Perform Colorization
#code:
import cv2 
import numpy as np 
import matplotlib.pyplot as plt 
# Load the colorization model  
net = cv2.dnn.readNetFromCaffe('colorization_deploy_v2.prototxt','colorization_release_v2.caffemodel') 
# Load cluster centers 
pts_in_hull = np.load('pts_in_hull.npy', allow_pickle=True) 
# Populate cluster centers as 1*1 convolution kernel 
class8 = net.getLayerId('class8_ab') 
conv8 = net.getLayerId('conv8_313_rh') 
pts_in_hull = pts_in_hull.transpose().reshape(2,313,1,1) 
net.getLayer(class8).blobs = [pts_in_hull.astype(np.float32)] 
net.getLayer(conv8).blobs = [np.full([1,313],2.606, np.float32)]
# read the input image 
image = cv2.imread('dog.jpeg') 
# convert to grayscale 
gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) 
# convert to RGB 
rgb_image = cv2.cvtColor(gray_image, cv2.COLOR_GRAY2RGB) 
 
# Normalize the image 
normalized_image = rgb_image.astype('float32') / 255.0 
# Convert image to lab 
lab_image = cv2.cvtColor(normalized_image, cv2.COLOR_RGB2Lab) 
# Resize the lightness channel to network input size 
resized_l_channel = cv2.resize(lab_image[:,:,0], (224,224)) 
resized_l_channel -= 50   # subtract 50 from mean-centering 
 
# predict the a and b channels 
net.setInput(cv2.dnn.blobFromImage(resized_l_channel)) 
pred = net.forward()[0,:,:,:].transpose((1,2,0)) 
 
# resize the predicted 'ab' image to the same dimensions as our input image 
pred_resized = cv2.resize(pred, (image.shape[1],image.shape[0])) 
 
# concentrate the original L channel with the predicted 'ab' channel 
colorized_image = np.concatenate((lab_image[:,:,0][:,:,np.newaxis],pred_resized), axis=2) 
 
# convert the output image from lab to bgr 
colorized_image = cv2.cvtColor(colorized_image, cv2.COLOR_Lab2BGR) 
 
# clip any values that fall outside the range [0,1] 
colorized_image = np.clip(colorized_image,0,1)
# convert the image to 8 bit and save 
colorized_image = (255*colorized_image).astype('uint8') 
cv2.imwrite('path_to_output/colorized_image.jpg', colorized_image) 
# convert the colorized image from BGR to RGB for matplotlib display 
colorized_image_rgb = cv2.cvtColor(colorized_image, cv2.COLOR_BGR2RGB) 
# show both the original grayscale and colorized image using matplotlib 
plt.figure(figsize=(14,7)) 
plt.subplot(1,2,1) 
plt.imshow(gray_image, cmap='gray') 
plt.title('Original Grayscale Image') 
plt.axis('off') 
plt.subplot(1,2,2) 
plt.imshow(colorized_image_rgb) 
plt.title('Colorized Image') 
plt.axis('off') 
plt.show()

prac 9
#Aim) Perform Text Detection and Recognition
#code:
import pytesseract 
import cv2 
from pytesseract import Output 
# Specify the tesseract executable path 
pytesseract.pytesseract.tesseract_cmd = r'C:\Program Files\Tesseract-OCR\tesseract.exe' 
 
# Load the image 
image = cv2.imread('txt.png') 
original_image = image.copy()  # Make a copy for displaying the original later 
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) 
 
# Use Opencv to find text blocks (simple thresholding) 
_, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV) 
 
# Dilate to connect text characters 
kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5,5)) 
dilate = cv2.dilate(thresh, kernel, iterations = 3) 
 
# Find Contours 
contours, _ = cv2.findContours(dilate, cv2.RETR_EXTERNAL, 
cv2.CHAIN_APPROX_SIMPLE) 
 
# Sort Contours by their y-coordinate first, then x-coordinate 
lines = sorted(contours, key=lambda ctr: 
(cv2.boundingRect(ctr)[1],cv2.boundingRect(ctr)[0])) 
 
# Go through each contour, crop and read the text 
for contour in lines: 
    x,y,w,h = cv2.boundingRect(contour) 
    # Make sure the contour area is a reasonable size 
    if w*h > 50: 
        roi = image[y:y+h, x:x+w] 
        text = pytesseract.image_to_string(roi, lang='eng', config= '--psm 6') 
        cv2.putText(image, text, (x,y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0,255,0), 1) 
        cv2.rectangle(image, (x,y), (x+w,y+h), (0,255,0), 1) 
        print(text) 
         
import matplotlib.pyplot as plt 
original_image_rgb = cv2.cvtColor(original_image, cv2.COLOR_BGR2RGB) 
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB) 
 
# Display the original image 
plt.figure(figsize=(10,10)) 
plt.subplot(1,2,1) 
plt.imshow(original_image_rgb) 
plt.title('Original Image')
plt.axis('off')
# Display the image with detected text 
plt.subplot(1,2,2) 
plt.imshow(image_rgb) 
plt.title('Image With Text') 
plt.axis('off') 
plt.show()

prac 10
#Aim: Perform Image matting and compositing
#code:
import cv2 
import numpy as np 
import matplotlib.pyplot as plt 
# Function to create a mask for the foreground 
def create_mask_for_foreground(fg_image): 
    # Convert to HSV (Hue, Saturation, Value) color space for easier color segmentation 
    fg_hsv = cv2.cvtColor(fg_image, cv2.COLOR_BGR2HSV) 
    # Define the range of green color in HSV 
    lower_green = np.array([36,25,25]) 
    upper_green = np.array([86,255,255]) 
    # Threshold the HSV image to get only green colors 
    mask = cv2.inRange(fg_hsv, lower_green, upper_green) 
    # Invert the mask to get the foreground  
    foreground_mask = cv2.bitwise_not(mask)
      # Apply a series of dilations and erosions to remove any small blobs left in the mask 
    kernel = np.ones((3,3), np.uint8) 
    foreground_mask = cv2.morphologyEx(foreground_mask, cv2.MORPH_OPEN, kernel, 
iterations = 2) 
    foreground_mask = cv2.dilate(foreground_mask, kernel, iterations = 4) 
    return foreground_mask 
 
# Functions to apply mask to foreground and composite with new background 
def composite_fg_with_new_bg(fg_image, bg_image, fg_mask): 
    # Resize the background to match the size of the foreground 
    bg_resized = cv2.resize(bg_image, (fg_image.shape[1],fg_image.shape[0])) 
    # Normalized the mask to be in the range [0,1] for blending 
    fg_mask_normalized = fg_mask / 255.0 
    # prepare the foreground and background for blending 
    fg_prepared = cv2.bitwise_and(fg_image, fg_image, mask=fg_mask) 
    bg_prepared = cv2.bitwise_and(bg_resized, bg_resized, mask=cv2.bitwise_not(fg_mask)) 
    # composite the images by adding them together 
    composite_image = cv2.add(fg_prepared, bg_prepared) 
    return composite_image 
# Load the foreground image (with a green screen background) 
foreground_image_path = 'green.png' 
foreground = cv2.imread(foreground_image_path) 
background_image_path = 'bg.png' 
background = cv2.imread(background_image_path) 
if background is None: 
    # If the background image is not available, use plain white background 
    background = np.full(foreground.shape, 255, dtype=foreground.dtype) 
# Create the mask for the foreground image  
foreground_mask = create_mask_for_foreground(foreground) 
# Composite the foreground with the new background 
composite_image = composite_fg_with_new_bg(foreground, background, foreground_mask) 
# Convert images from BGR to RGB for displaying 
foreground_rgb = cv2.cvtColor(foreground, cv2.COLOR_BGR2RGB) 
foreground_mask_rgb = cv2.cvtColor(foreground_mask, cv2.COLOR_BGR2RGB) 
composite_image_rgb = cv2.cvtColor(composite_image, cv2.COLOR_BGR2RGB) 
# Display the original image, the mask and the final composite image 
plt.figure(figsize=(18,6)) 
# Display the original image 
plt.subplot(1,3,1) 
plt.imshow(foreground_rgb) 
plt.title('Original Image') 
plt.axis('off') 
# Display the mask image 
plt.subplot(1,3,2) 
plt.imshow(foreground_mask_rgb) 
plt.title('Foreground Mask') 
plt.axis('off') 
# Display the composite image 
plt.subplot(1,3,3) 
plt.imshow(composite_image_rgb) 
plt.title('Composite Image') 
plt.axis('off') 
# Show the matplotlib plot 
plt.show()









IP Practicals
Practical 2A
i. Program to perform Image negation.
Code:
clc;
clear all;
A = imread('D:\MSC IT\Part I\Sem II\Image 
Processing\IP_Practical\IP_Practical_Images\negimg.jpg');
subplot(2,1,1);
imshow(A);
title('Orignial Image');
R = A(:,:,1); 
G = A(:,:,2); 
B = A(:,:,3);
[row col]=size(A);
for x=1:row
 for y=1:col
 R(x,y)=255-R(x,y);
 G(x,y)=255-G(x,y);
 B(x,y)=255-B(x,y);
 end
end
A(:,:,1)=R; 
A(:,:,2)=G; 
A(:,:,3)=B;
subplot(2,1,2);
imshow(A);
title('Image after negation');

ii. Program to perform threshold on an image.
Code:
clc;
a=imread('D:\MSC IT\Part I\Sem II\Image Processing\IP_Practical\Honey.jpg');
b=double(a); 
[m,n]=size(b); 
T=100; 
for i=1:m
 for j=1:n 
 if(b(i,j)<T)
 c(i,j)=0;
 else
 c(i,j)=255;
 end
 end
end 
imshow(uint8(c));

iii. Program to perform Log transformation.
Code:
//LOG
a=imread('D:\MSC IT\Part I\Sem II\Image Processing\IP_Practical\Honey.jpg');
b=rgb2gray(a);
//Log operator
c=edge(b,'log');
imshow(c)

iv. Power-law transformations
Code:
//Power Law transformation
clear all;
clc;
close all;
i=imread('D:\MSC IT\Part I\Sem II\Image 
Processing\IP_Practical\IP_Practical_Images\flower.jpg');
subplot(2,1,1);
imshow(i);
title('Original Image');
i=im2double(i);
c=1;
[row col]=size(i);
for x=1:row
 for y=1:col
 i(x,y)=c*i(x,y)^0.5; //1.5
 end
end
i=im2uint8(i);
subplot(2,1,2);
imshow(i);
title('Image after power-law transformation');

v. Piecewise linear transformations
a. Contrast Stretching
Code:
clc
a = imread ("D:\MSC IT\Part I\Sem II\Image 
Processing\IP_Practical\IP_Practical_Images\lena.png");
a = rgb2gray ( a ) ;
b = double ( a ) *0.5;
b = uint8 ( b );
c = double ( b ) *2;
c = uint8 ( c );
subplot(1,3,1)
imshow(a) ;
title ( "Original Image " )
subplot(1,3,2)
imshow(b) ;
title ( "Decrease in Contrast" )
subplot(1,3,3)
imshow(c) ;
title ( "Increase in Contrast")

b. Gray-level slicing with and without background
Code with background:
clc;
clear all;
a=imread('D:\MSC IT\Part I\Sem II\Image 
Processing\IP_Practical\IP_Practical_Images\lena.png');
a1=58; // This value is user defined
b1=158; // This value is user defined
[r,c]=size(a);
figure(2);
subplot(2,1,1);
imshow(a);
for i=1:r
 for j=1:c
 if (a(i,j)>a1 & a(i,j)<b1)
 x(i,j)=255;
 else
 x(i,j)=a(i,j);
 end
 end
end
x=uint8(x);
subplot(2,1,2);
title('Gray level slicing with background')
imshow(x);
Code without background:
clc;
clear all;
a=imread('D:\MSC IT\Part I\Sem II\Image 
Processing\IP_Practical\IP_Practical_Images\lena.png');
a1=50; // This value is user defined
b1=150; // This value is user defined
[r,c]=size(a);
figure(1)
subplot(2,1,1);
imshow(a);
for i=1:r
 for j=1:c
 if (a(i,j)>a1 & a(i,j)<b1)
 x(i,j)=255;
 else
 x(i,j)=0;
 end
 end
end
x=uint8(x);
subplot(2,1,2);
title('Gray level slicing without background');
imshow(x);

c. Bit-plane slicing
Code:
clc;
clear all;
f=imread('D:\MSC IT\Part I\Sem II\Image 
Processing\IP_Practical\IP_Practical_Images\lenag.jpeg');
f=double(f);
[r,c]=size(f);
com=[128 64 32 16 8 4 2 1];
for k=1:1:length(com);
 for i=1:r
 for j=1:c
 new(i,j)=bitand(f(i,j),com(k));
 end
 subplot(2,4,k);
 imshow(new);
end
end

extra 
from ultralytics import YOLO
import cv2

model = YOLO("../Weights/yolov9c.pt")
results = model("messi cop.png", show=True)
cv2.waitkey(0)
