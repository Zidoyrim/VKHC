{\rtf1\ansi\ansicpg1252\cocoartf2761
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\froman\fcharset0 TimesNewRomanPSMT;\f1\froman\fcharset0 Times-Roman;}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;}
\paperw11900\paperh16840\margl1440\margr1440\vieww16880\viewh8400\viewkind0
\deftab720
\pard\pardeftab720\sa213\partightenfactor0

\f0\fs32 \cf0 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec2 Decision tree\
\
import pandas as pd\
import numpy as np\
import matplotlib.pyplot as plt\
from sklearn import tree as sk_tree\
\
# Step 1: Parse the dataset\
data = \{\
    'Age': ['<=30', '<=30', '31-40', '>40', '>40', '>40', '31-40', '<=30', '<=30', '>40', '<=30', '31-40', '31-40', '>40'],\
    'Income': ['High', 'High', 'High', 'Medium', 'Low', 'Low', 'Low', 'Medium', 'Low', 'Medium', 'Medium', 'Medium', 'High', 'Medium'],\
    'Student': ['No', 'No', 'No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No'],\
    'Credit Rating': ['Fair', 'Excellent', 'Fair', 'Fair', 'Fair', 'Excellent', 'Excellent', 'Fair', 'Fair', 'Fair', 'Excellent', 'Excellent', 'Fair', 'Excellent'],\
    'Buys Computer': ['No', 'No', 'Yes', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'Yes', 'Yes', 'Yes', 'No']\
\}\
\
df = pd.DataFrame(data)\
\
# Encode the categorical variables\
df_encoded = df.apply(lambda x: pd.factorize(x)[0])\
\
# Fit the decision tree classifier using Gini impurity\
clf_gini = sk_tree.DecisionTreeClassifier(criterion='gini')\
clf_gini = clf_gini.fit(df_encoded.iloc[:, :-1], df_encoded['Buys Computer'])\
\
# Convert the feature names from Index to list\
feature_names = df.columns[:-1].tolist()\
\
# Convert the class names to a list\
class_names = df['Buys Computer'].unique().tolist()\
\
# Plot the decision tree\
plt.figure(figsize=(20, 10))\
sk_tree.plot_tree(clf_gini, feature_names=feature_names, class_names=class_names, filled=True)\
plt.show()\
\
# Function to print Gini impurity and chosen attribute at each split\
def print_gini_and_splits(tree, feature_names):\
    tree_ = tree.tree_\
    feature_name = [\
        feature_names[i] if i != sk_tree._tree.TREE_UNDEFINED else "undefined!"\
        for i in tree_.feature\
    ]\
\
    print("Decision tree splits and Gini impurities:")\
    for i in range(tree_.node_count):\
        if tree_.children_left[i] != sk_tree._tree.TREE_LEAF:\
            print(f"Node \{i\} (Gini: \{tree_.impurity[i]:.4f\}): split on feature '\{feature_name[i]\}'")\
        else:\
            print(f"Node \{i\} (Gini: \{tree_.impurity[i]:.4f\}): leaf node")\
\
print_gini_and_splits(clf_gini, feature_names)\
\
# Example test sample\
test_sample = \{\
    'Age': '<=30',\
    'Income': 'Medium',\
    'Student': 'Yes',\
    'Credit Rating': 'Fair'\
\}\
\
# Encode the test sample based on the dataset encoding\
encoded_sample = [\
    pd.factorize(df[col])[0][df[col].tolist().index(test_sample[col])] \
    for col in test_sample\
]\
\
# Predict using sklearn decision tree\
sklearn_prediction = clf_gini.predict([encoded_sample])\
decoded_prediction = pd.factorize(df['Buys Computer'])[1][sklearn_prediction[0]]\
print("Prediction for sklearn decision tree:", decoded_prediction)\
\
\
\pard\pardeftab720\sa213\qc\partightenfactor0
\cf0 \strokec2 KNN
\f1\fs29\fsmilli14667 \
\pard\pardeftab720\sa213\partightenfactor0

\f0\fs32 \cf0 \strokec2 Code:-
\f1\fs29\fsmilli14667 \

\f0\fs32 # Step 1: Import necessary libraries
\f1\fs29\fsmilli14667 \

\f0\fs32 import pandas as pd\
import matplotlib.pyplot as plt\
import seaborn as sns\
from sklearn.model_selection import train_test_split\
from sklearn.preprocessing import StandardScaler\
from sklearn.neighbors import KNeighborsClassifier\
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\
from mpl_toolkits.mplot3d import Axes3D\
\
# Step 2: Load and display the sample data\
data = \{\
    'Age': [19, 21, 20, 23, 31, 22, 35, 25, 23, 64, 30, 67, 35, 58, 24],\
    'Annual Income (k$)': [15, 15, 16, 16, 17, 17, 18, 18, 19, 19, 20, 20, 21, 21, 22],\
    'Spending Score (1-100)': [39, 81, 6, 77, 40, 76, 6, 94, 3, 72, 79, 65, 76, 76, 94],\
    'Segment': [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1]  # 0: Low-value, 1: High-value\
\}\
\
df = pd.DataFrame(data)\
print("Sample Data:")\
print(df.head())\
\
# Step 3: Data Preprocessing\
X = df[['Age', 'Annual Income (k$)', 'Spending Score (1-100)']]\
y = df['Segment']\
\
scaler = StandardScaler()\
X_scaled = scaler.fit_transform(X)\
\
# Step 4: Train-Test Split\
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\
\
# Step 5: Apply KNN Algorithm\
knn = KNeighborsClassifier(n_neighbors=3)\
knn.fit(X_train, y_train)\
y_pred = knn.predict(X_test)\
\
# Step 6: Evaluation\
print("\\nConfusion Matrix:")\
print(confusion_matrix(y_test, y_pred))\
print("\\nClassification Report:")\
print(classification_report(y_test, y_pred))\
print("\\nAccuracy Score:")\
print(accuracy_score(y_test, y_pred))\
\
# Step 7: Classify new user input\
new_user_data = \{'Age': [27], 'Annual Income (k$)': [23], 'Spending Score (1-100)': [60]\}\
new_user_df = pd.DataFrame(new_user_data)\
new_user_scaled = scaler.transform(new_user_df)\
\
new_user_segment = knn.predict(new_user_scaled)\
new_user_df['Segment'] = new_user_segment\
print("\\nNew User Data Prediction:")\
print(new_user_df)\
\
# Visualization: Scatter plot of the customer segments\
plt.figure(figsize=(10, 6))\
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Segment', data=df, palette='Set1', marker='o', s=100, label='Existing Data')\
sns.scatterplot(x='Annual Income (k$)', y='Spending Score (1-100)', hue='Segment', data=new_user_df, palette='Set2', marker='X', s=200, label='New User Data')\
plt.title('Customer Segments with New User Input')\
plt.xlabel('Annual Income (k$)')\
plt.ylabel('Spending Score (1-100)')\
plt.legend()\
plt.show()\
\
# Visualization: 3D plot for KNN decision boundaries and customer segments including new user input\
fig = plt.figure(figsize=(10, 6))\
ax = fig.add_subplot(111, projection='3d')\
\
# Plot the existing data with original values\
ax.scatter(X['Age'], X['Annual Income (k$)'], X['Spending Score (1-100)'], c=y, cmap='Set1', s=50, label='Existing Data')\
\
# Plot the new user input with original values\
ax.scatter(new_user_df['Age'], new_user_df['Annual Income (k$)'], new_user_df['Spending Score (1-100)'], c='green', marker='X', s=200, label='New User Data')\
ax.set_xlabel('Age')\
ax.set_ylabel('Annual Income (k$)')\
ax.set_zlabel('Spending Score (1-100)')\
plt.title('3D Plot of Customer Segments with New User Input')\
ax.legend()\
plt.show()\
\
\
\pard\pardeftab720\sa213\qc\partightenfactor0
\cf0 \strokec2 Na\'efve Bayes
\f1\fs29\fsmilli14667 \
\pard\pardeftab720\sa213\partightenfactor0

\f0\fs32 \cf0 \strokec2 Code:-
\f1\fs29\fsmilli14667 \

\f0\fs32 import numpy as np\
import matplotlib.pyplot as plt\
import pandas as pd\
\
# Importing the dataset\
dataset = pd.read_csv('Social_Network_Ads.csv')\
X = dataset.iloc[:, [2, 3]].values\
y = dataset.iloc[:, 4].values\
\
# Splitting the dataset into the Training set and Test set\
from sklearn.model_selection import train_test_split\
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\
\
# Feature Scaling\
from sklearn.preprocessing import StandardScaler\
sc = StandardScaler()\
X_train = sc.fit_transform(X_train)\
X_test = sc.transform(X_test)\
\
# Fitting classifier to the Training set\
from sklearn.naive_bayes import GaussianNB\
classifier = GaussianNB()\
classifier.fit(X_train, y_train)\
\
# Predicting the Test set results\
y_pred = classifier.predict(X_test)\
\
# Making the Confusion Matrix\
from sklearn.metrics import confusion_matrix\
cm = confusion_matrix(y_test, y_pred)\
print("Confusion Matrix:\\n", cm)\
\
# Visualising the Training set results\
from matplotlib.colors import ListedColormap\
X_set, y_set = X_train, y_train\
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))\
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\
             alpha=0.75, cmap=ListedColormap(['#FF0000', '#00FF00']))  # Using RGB hex codes for red and green\
plt.xlim(X1.min(), X1.max())\
plt.ylim(X2.min(), X2.max())\
for i, j in enumerate(np.unique(y_set)):\
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\
                c=ListedColormap(['#FF0000', '#00FF00'])(i), label=j)  # Using hex codes for colors\
plt.title('Naive Bayes (Training set)')\
plt.xlabel('Age')\
plt.ylabel('Estimated Salary')\
plt.legend()\
plt.show()\
\
# Visualising the Test set results\
X_set, y_set = X_test, y_test\
X1, X2 = np.meshgrid(np.arange(start=X_set[:, 0].min() - 1, stop=X_set[:, 0].max() + 1, step=0.01),\
                     np.arange(start=X_set[:, 1].min() - 1, stop=X_set[:, 1].max() + 1, step=0.01))\
plt.contourf(X1, X2, classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape),\
             alpha=0.75, cmap=ListedColormap(['#FF0000', '#00FF00']))\
plt.xlim(X1.min(), X1.max())\
plt.ylim(X2.min(), X2.max())\
for i, j in enumerate(np.unique(y_set)):\
    plt.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\
                c=ListedColormap(['#FF0000', '#00FF00'])(i), label=j)\
plt.title('Naive Bayes (Test set)')\
plt.xlabel('Age')\
plt.ylabel('Estimated Salary')\
plt.legend()\
plt.show()
\f1\fs29\fsmilli14667 \
\
\
\
\pard\pardeftab720\sa213\qc\partightenfactor0

\f0\fs32 \cf0 \strokec2 Hierarchical Clustering\
\
\pard\pardeftab720\sa213\partightenfactor0
\cf0 \strokec2 Code:-
\f1\fs29\fsmilli14667 \

\f0\fs32 import pandas as pd\
import numpy as np\
from sklearn.cluster import AgglomerativeClustering\
from sklearn.model_selection import train_test_split\
from sklearn.ensemble import RandomForestClassifier\
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\
from sklearn.datasets import load_iris\
import matplotlib.pyplot as plt\
from scipy.cluster.hierarchy import dendrogram, linkage\
\
# Load the Iris dataset\
iris = load_iris()\
X = iris.data\
y = iris.target\
\
# Step 1: Hierarchical Clustering with different Linkage Methods and Draw dendrograms\
n_clusters = 3  # Number of clusters\
linkage_methods = ['ward', 'single', 'complete']  # Different linkage methods\
cluster_labels = []\
\
# Define figure for dendrograms\
plt.figure(figsize=(15, 5))\
\
for i, linkage_method in enumerate(linkage_methods):\
    # Perform clustering\
    labels = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage_method).fit_predict(X)\
    cluster_labels.append(labels)\
\
    # Create a dendrogram for the current linkage method\
    dendrogram_data = linkage(X, method=linkage_method)\
    \
    # Create subplots for each dendrogram\
    plt.subplot(1, len(linkage_methods), i + 1)\
    dendrogram(dendrogram_data, orientation='top', labels=labels)\
    plt.title(f"\{linkage_method.capitalize()\} Linkage Dendrogram")\
    plt.xlabel('Samples')\
    plt.ylabel('Distance')\
\
# Show the dendrograms\
plt.show()\
\
# Plot clustering results for different linkage methods\
plt.figure(figsize=(15, 5))\
for i, linkage_method in enumerate(linkage_methods):\
    plt.subplot(1, len(linkage_methods), i + 1)\
    scatter = plt.scatter(X[:, 0], X[:, 1], c=cluster_labels[i], cmap='viridis',\
                          label=f'Clusters (\{linkage_method.capitalize()\} Linkage)')\
    plt.title(f"\{linkage_method.capitalize()\} Linkage")\
\
# Add legend to scatter plots\
plt.legend(handles=scatter.legend_elements()[0], labels=[f'Cluster \{i\}' for i in range(n_clusters)])\
\
# Show the scatter plots\
plt.show()\
\
# Step 2: Feature Engineering (Using cluster assignment as a feature)\
X_with_cluster = np.column_stack((X, cluster_labels[-1]))  # Using complete linkage\
\
# Step 3: Classification\
X_train, X_test, y_train, y_test = train_test_split(X_with_cluster, y, test_size=0.2, random_state=42)\
classifier = RandomForestClassifier(n_estimators=100, random_state=42)\
classifier.fit(X_train, y_train)\
\
# Step 4: Prediction\
y_pred = classifier.predict(X_test)\
\
# Step 5: Test Score and Confusion Matrix\
accuracy = accuracy_score(y_test, y_pred)\
conf_matrix = confusion_matrix(y_test, y_pred)\
\
# Generate classification report with zero_division parameter\
classification_rep = classification_report(y_test, y_pred, zero_division=0)\
\
# Print cluster descriptions\
cluster_descriptions = \{\
    'ward': 'Clusters based on Ward linkage interpretation.',\
    'single': 'Clusters based on Single linkage interpretation.',\
    'complete': 'Clusters based on Complete linkage interpretation.'\
\}\
for method in linkage_methods:\
    print(f"Cluster Descriptions (\{method.capitalize()\} Linkage):")\
    print(cluster_descriptions[method.lower()])  # Convert to lowercase for dictionary access\
\
# Print accuracy, confusion matrix, and classification report\
print("Accuracy:", accuracy)\
print("Confusion Matrix:\\n", conf_matrix)\
print("Classification Report:\\n", classification_rep)\
\
# Show final plot\
plt.show()
\f1\fs29\fsmilli14667 \
\pard\pardeftab720\sa213\qc\partightenfactor0
\cf0 \strokec2 \
\
\pard\pardeftab720\sa213\qc\partightenfactor0

\f0\fs32 \cf0 KMeans Clustering
\f1\fs29\fsmilli14667 \
\pard\pardeftab720\sa213\partightenfactor0

\f0\fs32 \cf0 \strokec2 Code:-
\f1\fs29\fsmilli14667 \

\f0\fs32 import numpy as np\
import pandas as pd\
import matplotlib.pyplot as plt\
from sklearn.datasets import load_iris\
from sklearn.model_selection import train_test_split\
from sklearn.cluster import KMeans\
from sklearn.metrics import classification_report, confusion_matrix\
\
# Load the Iris dataset\
iris = load_iris()\
X = iris.data[:, :2]  # Select only the features (sepal length and sepal width)\
y = iris.target\
\
# Split dataset into training and testing\
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\
\
# Initialize K-Means clustering with the number of clusters equal to the number of classes\
n_clusters = len(np.unique(y))\
kmeans = KMeans(n_clusters=n_clusters, random_state=42)\
\
# Fit K-Means clustering to the training data\
kmeans.fit(X_train)\
\
# Assign cluster labels to data points in the test set\
cluster_labels = kmeans.predict(X_test)\
\
# Assign class labels to clusters based on the most frequent class label in each cluster\
cluster_class_labels = []\
for i in range(n_clusters):\
    cluster_indices = np.where(cluster_labels == i)[0]\
    if len(cluster_indices) > 0:\
        cluster_class_labels.append(np.bincount(y_test[cluster_indices]).argmax())\
    else:\
        cluster_class_labels.append(-1)  # In case a cluster has no test points, default label\
\
# Assign cluster class labels to data points in the test set\
y_pred = np.array([cluster_class_labels[cluster_labels[i]] for i in range(len(X_test))])\
\
# Evaluate the classifier's performance\
print("Confusion Matrix:")\
print(confusion_matrix(y_test, y_pred))\
print("\\nClassification Report:")\
print(classification_report(y_test, y_pred))\
\
# Visualize the dataset and cluster centers\
plt.figure(figsize=(10, 6))\
\
# Plot the training data points\
plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', label='Training Data')\
\
# Plot testing data\
plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap='viridis', marker='x', s=100, label='Testing Data')\
\
# Plot cluster centers\
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], c='red', marker='o', s=100, label='Cluster Centers')\
\
plt.xlabel('Sepal Length (cm)')\
plt.ylabel('Sepal Width (cm)')\
plt.title('K-Means Clustering with Class Labels on Iris Dataset')\
plt.legend()\
plt.show()
\f1\fs29\fsmilli14667 \
\pard\pardeftab720\sa213\partightenfactor0
\cf0 \strokec2 \
}